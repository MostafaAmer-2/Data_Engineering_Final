{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.experimental import enable_iterative_imputer  \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.core.defchararray import add\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0) \n",
    "df = pd.read_csv(\"data/Android/googleplaystore.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Duplicate along the App Column subset and dropna for columns with small percentage of missing value\n",
    "df.drop_duplicates(subset='App',inplace = True)\n",
    "df.dropna(subset=['Type', 'Content Rating','Current Ver','Android Ver'],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge Category and Genre in one column\n",
    "df_ex = ((((df.Category.str.lower()).str.split(\"_\")).str.join(' ')).str.split(\"and\")).str.join('&')\n",
    "df_gx = ((df.Genres.str.lower()).str.split(\";\"))\n",
    "listx=[]\n",
    "for ex,gx in zip(df_ex,df_gx):\n",
    "    if ex != gx[0]:\n",
    "        if not (ex in gx[0]):\n",
    "            gx.append(ex)\n",
    "            if \"educational\" in gx:\n",
    "                gx.remove(\"educational\")\n",
    "                gx.append(\"education\")\n",
    "    listx.append(np.unique(gx))\n",
    "df[\"Genres\"] = np.asarray(listx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Column Category, Price, Last Updated,Current Ver, Android Ver for there irrelvance\n",
    "df = df.drop([\"Category\",\"Price\",\"Last Updated\",\"Current Ver\",\"Android Ver\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct and convert size column to int\n",
    "def change_size(size):\n",
    "    if 'M' in size:\n",
    "        x = size[:-1]\n",
    "        x = float(x)*1000000\n",
    "        return(x)\n",
    "    elif 'k' == size[-1:]:\n",
    "        x = size[:-1]\n",
    "        x = float(x)*1000\n",
    "        return(x)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df[\"Size\"] = df[\"Size\"].map(change_size)\n",
    "\n",
    "df.Size.fillna(method = 'ffill', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert installs to int\n",
    "df['Installs'] = [int(i[:-1].replace(',','')) for i in df['Installs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----Remove unrated and label code the content rating\n",
    "df = df.drop(df[df[\"Content Rating\"]==\"Unrated\"].index)\n",
    "RatingL = df['Content Rating'].unique()\n",
    "RatingDict = {}\n",
    "for i in range(len(RatingL)):\n",
    "    RatingDict[RatingL[i]] = i\n",
    "df['Content Rating'] = df['Content Rating'].map(RatingDict).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean Type\n",
    "def type_clean(types):\n",
    "    if \"Free\" in types:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "df['Type'] = df['Type'].map(type_clean).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Reviews to int\n",
    "df['Reviews'] = df['Reviews'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert non-null Ratings to float\n",
    "df['Rating'] = df['Rating'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix indexing\n",
    "new_index = np.arange(0,len(df))\n",
    "df = df.set_index(new_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One Hot encode all genres\n",
    "mlb = MultiLabelBinarizer()\n",
    "df = df.join(pd.DataFrame(mlb.fit_transform(df.pop('Genres')),\n",
    "                          columns=mlb.classes_,\n",
    "                          index=df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter('Rating','Reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter('Rating','Size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter('Rating','Installs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "df_scaled = df.copy().dropna()\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "df_scaled[['Rating','Reviews']] = scaler.fit_transform(df_scaled[['Rating','Reviews']])\n",
    "df_scaled[['Rating','Reviews']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled[['Rating','Size']] = scaler.fit_transform(df_scaled[['Rating','Size']])\n",
    "df_scaled[['Rating','Size']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled[['Rating','Installs']] = scaler.fit_transform(df_scaled[['Rating','Installs']])\n",
    "df_scaled[['Rating','Installs']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.plot.scatter('Rating','Reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.plot.scatter('Rating','Size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.plot.scatter('Rating','Installs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_a = df_scaled['Rating'].values.reshape(-1,1) \n",
    "X2_a = df_scaled['Reviews'].values.reshape(-1,1)\n",
    "# values method takes a column from pandas dataframe and makes it into an array of values --\n",
    "# reshape arranges the extracted values in a numpy array with a shape we define --\n",
    "# an array shape is the number of rows and columns in the array\n",
    "# when we use parameter -1, it means we don't know the shape of the resulting array (e.g. X1)  -- \n",
    "# and numpy will figure it out based on length of array and whatever dimensions left\n",
    "# in the case of reshape(-1, 1) we tell numpy to figure out the number of rows (parameter -1) but we have one column (parameter 1)\n",
    "\n",
    "X_a = np.concatenate((X1_a,X2_a),axis=1) # define a NumPy array from the two arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_b = df_scaled['Rating'].values.reshape(-1,1) \n",
    "X2_b = df_scaled['Size'].values.reshape(-1,1)\n",
    "# values method takes a column from pandas dataframe and makes it into an array of values --\n",
    "# reshape arranges the extracted values in a numpy array with a shape we define --\n",
    "# an array shape is the number of rows and columns in the array\n",
    "# when we use parameter -1, it means we don't know the shape of the resulting array (e.g. X1)  -- \n",
    "# and numpy will figure it out based on length of array and whatever dimensions left\n",
    "# in the case of reshape(-1, 1) we tell numpy to figure out the number of rows (parameter -1) but we have one column (parameter 1)\n",
    "\n",
    "X_b = np.concatenate((X1_b,X2_b),axis=1) # define a NumPy array from the two arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_c = df_scaled['Rating'].values.reshape(-1,1) \n",
    "X2_c = df_scaled['Installs'].values.reshape(-1,1)\n",
    "# values method takes a column from pandas dataframe and makes it into an array of values --\n",
    "# reshape arranges the extracted values in a numpy array with a shape we define --\n",
    "# an array shape is the number of rows and columns in the array\n",
    "# when we use parameter -1, it means we don't know the shape of the resulting array (e.g. X1)  -- \n",
    "# and numpy will figure it out based on length of array and whatever dimensions left\n",
    "# in the case of reshape(-1, 1) we tell numpy to figure out the number ofdf_scaled rows (parameter -1) but we have one column (parameter 1)\n",
    "\n",
    "X_c = np.concatenate((X1_c,X2_c),axis=1) # define a NumPy array from the two arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import models\n",
    "from pyod.models.hbos import HBOS # histogram-based outlier detection module\n",
    "from pyod.models.cblof import CBLOF # cluster-based local outlier factor detection module\n",
    "from pyod.models.knn import KNN # k nearest neighbors module\n",
    "from pyod.models.lof import LOF # local outlier factor module\n",
    "\n",
    "\n",
    "random_state = np.random.RandomState(42)\n",
    "outliers_fraction = 0.05\n",
    "# Define four outlier detection tools to be compared\n",
    "# KNN uses the data point's distance to the farthest KNN for the outlier score\n",
    "# Average KNN uses the average score for the data point's k nearest neighbors as the outlier score\n",
    "\n",
    "# Create dictionary structure with model names and function calls\n",
    "classifiers = {\n",
    "#     'Histogram-base Outlier Detection (HBOS)': HBOS(contamination=outliers_fraction),    \n",
    "#     'Local Outlier Factor (LOF)':LOF(contamination=outliers_fraction),\n",
    "    #'Cluster-based Local Outlier Factor (CBLOF)':CBLOF(contamination=outliers_fraction,check_estimator=False, random_state=random_state),\n",
    "    'K Nearest Neighbors (KNN)': KNN(contamination=outliers_fraction),\n",
    "    #'Average KNN': KNN(method='mean',contamination=outliers_fraction)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# For Box-Cox Normalization\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "\n",
    "# # Define a grid with equally spaced cells using NumPy for visualization\n",
    "# xx , yy = np.meshgrid(np.linspace(0,1 , 200), np.linspace(0, 1, 200))\n",
    "\n",
    "# for iterator, call each model name and model function from the dictionary\n",
    "for i, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "    \n",
    "    # Fit the model in current iteration to data X which contains the two scaled attributes\n",
    "    clf.fit(X_a) \n",
    "    \n",
    "    # Predict raw anomaly score for entire dataset X using decision function\n",
    "    # Decision function is a conceptual separator between two classes in the dataset, in this case \"normal\" versus \"outlier\"\n",
    "    scores_pred = clf.decision_function(X_a) * -1 \n",
    "        \n",
    "    # Label each data point in X as outlier or inlier: label = 1 is for outlier and label = 0 is for inlier\n",
    "    y_pred = clf.predict(X_a)\n",
    "    \n",
    "\n",
    "y_pred = np.array(y_pred)    \n",
    "outlier_index = np.asarray(np.where(y_pred == 1)).flatten()\n",
    "# outlier_index.reshape((1, -1))\n",
    "\n",
    "for i in range(len(outlier_index)):\n",
    "    print(outlier_index[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "    \n",
    "    # Fit the model in current iteration to data X which contains the two scaled attributes\n",
    "    clf.fit(X_b) \n",
    "    \n",
    "    # Predict raw anomaly score for entire dataset X using decision function\n",
    "    # Decision function is a conceptual separator between two classes in the dataset, in this case \"normal\" versus \"outlier\"\n",
    "    scores_pred = clf.decision_function(X_b) * -1 \n",
    "        \n",
    "    # Label each data point in X as outlier or inlier: label = 1 is for outlier and label = 0 is for inlier\n",
    "    y_pred = clf.predict(X_b)\n",
    "    \n",
    "\n",
    "y_pred = np.array(y_pred)    \n",
    "outlier_index1 = np.asarray(np.where(y_pred == 1)).flatten()\n",
    "# outlier_index1.reshape((1, -1))\n",
    "\n",
    "for i in range(len(outlier_index1)):\n",
    "    print(outlier_index1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "    \n",
    "    # Fit the model in current iteration to data X which contains the two scaled attributes\n",
    "    clf.fit(X_c) \n",
    "    \n",
    "    # Predict raw anomaly score for entire dataset X using decision function\n",
    "    # Decision function is a conceptual separator between two classes in the dataset, in this case \"normal\" versus \"outlier\"\n",
    "    scores_pred = clf.decision_function(X_c) * -1 \n",
    "        \n",
    "    # Label each data point in X as outlier or inlier: label = 1 is for outlier and label = 0 is for inlier\n",
    "    y_pred = clf.predict(X_c)\n",
    "    \n",
    "\n",
    "y_pred = np.array(y_pred)    \n",
    "outlier_index2 = np.asarray(np.where(y_pred == 1)).flatten()\n",
    "# outlier_index2.reshape((1, -1))\n",
    "\n",
    "for i in range(len(outlier_index2)):\n",
    "    print(outlier_index2[i])\n",
    "    \n",
    "print(type(outlier_index2[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_index = np.concatenate((outlier_index, outlier_index1, outlier_index2))\n",
    "len(concat_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_index = np.unique(concat_index)\n",
    "len(concat_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(concat_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix indexing\n",
    "new_index = np.arange(0,len(df))\n",
    "df = df.set_index(new_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter('Rating','Reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "App                           0\n",
       "Rating                     1347\n",
       "Reviews                       0\n",
       "Size                          0\n",
       "Installs                      0\n",
       "                           ... \n",
       "travel & local                0\n",
       "trivia                        0\n",
       "video players & editors       0\n",
       "weather                       0\n",
       "word                          0\n",
       "Length: 61, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking for any missing values\n",
    "missing_values_count = df.isnull().sum()\n",
    "missing_values_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute the missing nans in Rating using multivariate imputation\n",
    "reg_imputer = IterativeImputer(BayesianRidge(), max_iter=5, random_state=0)\n",
    "subset_data = df.loc[:, 'Rating':'word']\n",
    "imputed_subset = pd.DataFrame(reg_imputer.fit_transform(subset_data), columns=subset_data.columns)\n",
    "df.loc[:,'Rating':'word'] = imputed_subset.loc[:,'Rating':'word']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "App                        0\n",
       "Rating                     0\n",
       "Reviews                    0\n",
       "Size                       0\n",
       "Installs                   0\n",
       "                          ..\n",
       "travel & local             0\n",
       "trivia                     0\n",
       "video players & editors    0\n",
       "weather                    0\n",
       "word                       0\n",
       "Length: 61, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking for any missing values\n",
    "missing_values_count = df.isnull().sum()\n",
    "missing_values_count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
